**Davi Bezerra Barros**

# 1. Introdu√ß√£o

A Ultrassonografia √© uma t√©cnica muito utilizada em diagn√≥sticos medicinais, que aproveita o efeito doppler na reflex√£o de ondas sonoras de alta frequ√™ncia - acima de 20kHz - para visualizar as estruturas internas de um organismo em tempo real. Por ser um m√©todo de baixo custo, n√£o invasivo e indolor, a ultrassonografia √© amplamente utilizada para avaliar uma grande variedade de √≥rg√£os do corpo humano, como:

- **Abd√¥men:** F√≠gado, ves√≠cula biliar, p√¢ncreas, ba√ßo, rins, bexiga e √≥rg√£os reprodutivos.
- **Pelve:** √ötero, ov√°rios, pr√≥stata e bexiga.
- **Mamas:** Para detectar n√≥dulos e outros problemas.
- **Cora√ß√£o:** Para avaliar o funcionamento do cora√ß√£o e identificar doen√ßas card√≠acas.
- **M√∫sculos e tend√µes:** Para diagnosticar les√µes.
- **Vasos sangu√≠neos:** Para avaliar o fluxo sangu√≠neo e identificar obstru√ß√µes.
- **Gesta√ß√£o:** Para acompanhar o desenvolvimento do feto, verificar a data prov√°vel do parto e identificar poss√≠veis complica√ß√µes.

![[k72.jpg|center]]

No entanto, a qualidade do exame depende da habilidade e experi√™ncia do profissional que o realiza, visto que eles precisam discriminar as propriedades ecogen√©ticas e caracter√≠sticas ruidosas inerentes aos tecidos observados. A qualidade da interpreta√ß√£o tamb√©m est√° ligada √† habilidade do radiologista em localizar √°reas anat√¥micas utilizando o transdutor, evidenciando a necessidade de se criar m√©todos e ferramentas para o treinamento destes profissionais.

# 2. Metodologia

## 2.1. Objetivo do Projeto:
Para abordar estas dificuldades, este projeto prop√µe a implementa√ß√£o de um sistema capaz de identificar e destacar √≥rg√£os abdominais em imagens de ultrassonografia utilizando *segmenta√ß√£o sem√¢ntica*, uma t√©cnica de vis√£o computacional. Para isso ser√° utilizado um modelo de deep learning baseado na arquitetura **U-Net**, que se mostrou eficaz em tarefas de segmenta√ß√£o de imagens m√©dicas. M√©tricas como ***IoU*(Intersection over Union)** e **Acur√°cia** ser√£o utilizadas para avaliar o desempenho do modelo.

A inspira√ß√£o e motiva√ß√£o para o desenvolvimento deste trabalho veio do artigo: "**Automated Deep Learning-Based Finger Joint Segmentation in 3-D Ultrasound Images With Limited Dataset**", dispon√≠vel no link: https://www.researchgate.net/publication/384148850_Automated_Deep_Learning-Based_Finger_Joint_Segmentation_in_3-D_Ultrasound_Images_With_Limited_Dataset
## 2.2. Segmenta√ß√£o sem√¢ntica

Segmenta√ß√£o de imagens, em vis√£o computacional, √© a tarefa de particionar uma imagem em v√°rias regi√µes, onde os pixels dessas regi√µes devem compartilhar certas caracter√≠sticas. H√° tr√™s grandes grupos de tarefas de segmenta√ß√£o de imagens: 

- Segmenta√ß√£o sem√¢ntica
- Segmenta√ß√£o de inst√¢ncias
- Segmenta√ß√£o pan√≥ptica (sem√¢ntica + inst√¢ncias)

O objetivo da segmenta√ß√£o sem√¢ntica √© categorizar *cada pixel* da imagem com a classe do objeto identificado. Isso √© conhecido como *predi√ß√£o densa*, pois cada pixel da imagem est√° sendo inferido individualmente.

![[Pasted image 20241119230146.png|center|500]]

Como os objetos que pertencem a mesma classe n√£o s√£o instanciados individualmente, o mapa de segmenta√ß√£o n√£o os identifica como objetos diferentes; **Esta √© a tarefa da segmenta√ß√£o de inst√¢ncias**.

### 2.2.1. Funcionamento 

O objetivo da segmenta√ß√£o sem√¢ntica, em ess√™ncia, √© receber como input uma imagem RGB (H, W, 3) ou em escala de cinza (H, W, 1) e produzir um mapa de segmenta√ß√£o onde cada pixel √© um valor inteiro que representa uma classe (H, W, 1). 

![[Pasted image 20241120022944.png|center]]

Como esta √© uma tarefa *categ√≥rica*, tanto as m√°scaras de treinamento quanto as m√°scaras previstas s√£o representadas por vetores one-hot-encoded com formato (H, W, C) onde C √© o n√∫mero de canais que representam as probabilidades das classes. Ao selecionar a maior probabilidade para cada pixel, o mapa retorna ao formato (H, W, 1) e o resultado pode ser observado ao sobrepor a imagem original:

![[Pasted image 20241120023929.png|center]]

## 2.3. Arquitetura da Rede

A arquitetura de rede escolhida para o projeto foi a **U-net**, uma rede totalmente convolucional proposta em 2015, especificamente para tarefas de segmenta√ß√£o de imagens biom√©dicas, mas que teve seu uso expandido para outras tarefas de segmenta√ß√£o devido ao √† sua efici√™ncia e simplicidade. Sua estrutura em formato de "U"(da√≠ o nome) foi criada de forma a permitir o treinamento com poucas imagens, ao utilizar m√©todos de data augmentation. Esta capacidade a torna ideal para a aplica√ß√£o no campo da biomedicina, onde muitas vezes n√£o √© poss√≠vel coletar uma grande amostra de imagens para treinamento.


![[Pasted image 20241120182648.png|center]]

### 2.3.1. Estrutura

A U-net √© composta por dois componentes principais: um caminho de *contra√ß√£o*(encoder) e um caminho de *expans√£o*(decoder), conectados por *skip connections* para permitir a localiza√ß√£o exata das caracter√≠sticas extra√≠das. 

#### 2.3.1.1. Caminho de contra√ß√£o:
O caminho de contra√ß√£o captura informa√ß√µes contextuais e caracter√≠sticas globais da imagem, enquanto reduz as suas dimens√µes e aumenta o n√∫mero de mapas de caracter√≠sticas. Cada bloco de contra√ß√£o √© composto por:

- **Convolu√ß√£o:** Duas camadas de convolu√ß√£o 3x3 ativadas por ReLU.
- **Downsampling:** Redu√ß√£o das dimens√µes com uma camada de *Max Pooling* 2x2.

![[Pasted image 20241120181802.png|center|500]]

O caminho de contra√ß√£o √© formado por 5 blocos convolucionais que extraem as caracter√≠sticas locais, seguidos por uma camada de *max pooling* para reduzir a dimensionalidade espacial enquanto mant√©m as caracter√≠sticas relevantes. A cada bloco o n√∫mero de filtros √© dobrado para capturar informa√ß√µes mais abstratas nas camadas mais profundas.

**Bloco 1:**
1. Recebe a imagem de entrada de formato (572, 572, 1) que ser√° processada pela U-net.
2. Duas camadas de convolu√ß√£o 3x3 e padding 'valid' s√£o aplicadas √† imagem, seguidas por uma camada de ativa√ß√£o ReLU, gerando **64** mapas de caracter√≠sticas.
3. Em seguida, √© aplicada uma camada de *Max Pooling 2x2* reduzindo os mapas de caracter√≠sticas √† metade de suas dimens√µes.

**Bloco 2:**
1. Recebe a imagem do bloco anterior
2. Aplica as mesmas camadas convolucionais, dobrando o n√∫mero de mapas para **128.**
3. Max pooling 2x2 reduzindo √† metade as dimens√µes do mapa ao sair da segunda camada convolucional.

**Bloco 3:**  Mesmo processo dos blocos 1 e 2, gerando **256** mapas de caracter√≠sticas.
**Bloco 4:** Mesmo processo dos blocos 1, 2 e 3, gerando **512** mapas de caracter√≠sticas.

**Bloco de conex√£o (Bottleneck):* *
1. Recebe os mapas da camada 4 e aplica 2 camadas de convolu√ß√£o 3x3, gerando **1024** mapas de caracter√≠stica.
2. Fornece a transi√ß√£o dos mapas gerados no caminho de contra√ß√£o, para o caminho de expans√£o.

#### 2.3.1.2. Caminho de expans√£o:

O caminho de expans√£o reconstr√≥i a imagem, mapeando as caracter√≠sticas aprendidas de volta para a resolu√ß√£o original. Ele utiliza t√©cnicas de  *upsampling* e concatena√ß√µes (*skip connections*) com os mapas de caracter√≠sticas dos blocos da camada de contra√ß√£o para preservar a localiza√ß√£o das informa√ß√µes extra√≠das. Cada bloco √© composto por: 

- **Upsampling:** Aumento da dimens√£o dos mapas de caracter√≠sticas utilizando convolu√ß√µes transpostas.
- **Concatena√ß√£o:** Adi√ß√£o dos mapas de caracter√≠sticas do bloco sim√©trico no caminho de contra√ß√£o.
- **Convolu√ß√£o:** Duas camadas convolucionais 3x3 com ativa√ß√£o ReLU.


![[Pasted image 20241120185657.png|center|500]]



**Bloco 6:**
1. Recebe os mapas comprimidos do bottleneck com 1024 mapas de caracter√≠sticas.
2. Realiza um _upsampling_ para dobrar as dimens√µes espaciais, reduzindo o n√∫mero de mapas de caracter√≠sticas para **512.**
3. Concatena os mapas gerados com os mapas correspondentes do Bloco 4, gerando 1024 mapas.
4. Aplica duas convolu√ß√µes 3√ó3 seguidas por ReLU, reduzindo os mapas para 512.

**Bloco 7:** Mesmo processo do bloco 6, reduz os mapas de caracter√≠sticas para **256**.

**Bloco 8:** Mesmo processo do bloco 7, reduz os mapas de caracter√≠sticas para **128**.

**Bloco 9(Sa√≠da):**
1. Recebe os mapas do Bloco 8 e realiza um upsampling, dobrando as dimens√µes espaciais e reduzindo os mapas para 64.
2. Concatena com os mapas correspondentes do bloco 1, gerando 128 mapas.
3. Aplica uma convolu√ß√£o 3x3 seguidas por ReLU, reduzindo os mapas para 64.
4. Aplica uma convolu√ß√£o 1x1 seguida por uma camada de ativa√ß√£o *Sigmoide*, reduzindo o n√∫mero de canais de caracter√≠sticas para a quantidade de classes desejada. Gera o mapa de segmenta√ß√£o final, no mesmo tamanho da imagem original, com cada pixel representando uma classe no formato de one-hot encoding.

Essa arquitetura garante que tanto as informa√ß√µes contextuais geradas pelo encoder quanto os detalhes espaciais das caracter√≠sticas sejam aproveitados.
# 3. Implementa√ß√£o do projeto

A seguir ser√£o descritas e demonstradas todas as etapas da implementa√ß√£o deste projeto, desde o pr√©-processamento dos dados at√© a visualiza√ß√£o e avalia√ß√£o dos resultados. Todo o c√≥digo e coment√°rios de desenvolvimento podem ser encontrados no notebook: https://colab.research.google.com/drive/1drz8sJ3-XT-IUotxwml6d1YeRC2AHLE0?usp=sharing
## 3.1. Aquisi√ß√£o de dados

O dataset escolhido para este trabalho foi o "US simulation & segmentation", dispon√≠vel no kaggle pelo link: https://www.kaggle.com/datasets/ignaciorlando/ussimandsegm

Este dataset foi desenvolvido com  imagens reais de ultrassonografia e imagens sint√©ticas geradas por um simulador baseado em ray-casting, a partir de dados volum√©tricos de **Tomografia Computadorizada**. As imagens representam √≥rg√£os abdominais, como f√≠gado, p√¢ncreas, adrenais, ves√≠cula biliar, ba√ßo, vasos sangu√≠neos e ossos, totalizando 694 imagens anotadas divididas entre os conjuntos reais e sint√©ticos.

**Conte√∫do**
O dataset est√° dividido entre imagens reais e imagens sint√©ticas, j√° separados em subconjuntos de treino e teste.

1. **Imagens Reais de Ultrassonografia:** Total de 617 imagens
   ![[Pasted image 20241120211024.png|center]]
	1. **Treino:
		- 404  imagens reais de ultrassonografia.
		- Nenhuma imagem possui anota√ß√£o no subconjunto de treino.
	2. **Teste:**
		 - 213 imagens reais de ultrassonografia.
		 - 61 m√°scaras de segmenta√ß√£o anotadas manualmente.

2. **Imagens Sint√©ticas de Ultrassonografia:** Total de 926 imagens
   ![[Pasted image 20241120211424.png|center]]
	1. **Treino:**
		-  633 imagens sint√©ticas de ultrassonografia.
	    - 633 m√°scaras de segmenta√ß√£o
    1. **Teste:**
		-  293 imagens sint√©ticas de ultrassonografia.
		- 293 m√°scaras de segmenta√ß√£o anotadas automaticamente.

**Anota√ß√µes**
As anota√ß√µes s√£o representadas por cores que identificam diferentes √≥rg√£os abdominais:

- **Violeta:** F√≠gado.
- **Amarelo:** Rins.
- **Azul:** P√¢ncreas.
- **Vermelho:** Vasos.
- **Azul claro:** Adrenais.
- **Verde:** Ves√≠cula biliar.
- **Branco:** Ossos.
- **Rosa:** Ba√ßo.
## 3.2. Implementa√ß√£o da U-Net

A arquitetura da rede foi implementada manualmente seguindo o artigo original "U-Net: Convolutional Networks for Biomedical Image Segmentation", dispon√≠vel no link: https://arxiv.org/pdf/1505.04597v1 

Para a modelagem do modelo, foi utilizado o framework **TensorFlow** com a **API Keras** da seguinte maneira: 

**importa√ß√£o dos pacotes**

```python
import tensorflow as tf
from tensorflow.keras import  Model
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import load_model
from tensorflow.keras.models import model_from_json
from tensorflow.keras.losses import SparseCategoricalCrossentropy

```

### 3.2.1. Fun√ß√µes e arquitetura

**Bloco convolucional:** Fun√ß√£o que encapsula opera√ß√µes repetidas de convolu√ß√£o 3√ó3, Batch Normalization, ativa√ß√£o ReLU e dropout.

```python
def ConvBlock(tensor, num_feature):

	x = tf.keras.layers.Conv2D(num_feature,
										(3,3),
										activation='relu',
										kernel_initializer='he_normal',
										padding='same')(tensor)
	
	x = tf.keras.layers.BatchNormalization()(x)
	x = tf.keras.layers.Dropout(0.1)(x)
	x = tf.keras.layers.Conv2D(num_feature,
										3,3),
										activation='relu',
										kernel_initializer='he_normal',
										padding='same')(x)
	x = tf.keras.layers.BatchNormalization()(x)
	return x
```
- Configura os pesos usando inicializa√ß√£o HeNormal.
<br>

**Bloco "Bottleneck":** 
```python
def ponte(tensor, num_feature):
  x = ConvBlock(tensor,num_feature)
  return x
```
- Conecta as se√ß√µes encoder e decoder.
- Aplica um bloco convolucional com o maior n√∫mero de filtros.
<br>

**Blocos encoder e decoder :** Aplicam as convolu√ß√µes, downsamplings, upsamplings, concatena√ß√µes e passam o tensor adiante.
```python
#Se√ß√£o encoder
def encoder_block(tensor, num_feature):
  x = ConvBlock(tensor, num_feature)
  p = tf.keras.layers.MaxPooling2D((2,2))(x)
  return x, p

#se√ß√£o decoder
def decoder_block(tensor,skip_connection, num_feature):
  x = tf.keras.layers.Conv2DTranspose(num_feature, (2,2), strides=(2,2), padding='same')(tensor) #recebe do bloco anterior e faz upsampling
  x = tf.keras.layers.concatenate([x, skip_connection])
  x = ConvBlock(x, num_feature)
  return x
```
**Se√ß√£o encoder:**
- Aplica o _ConvBlock_ para extra√ß√£o de caracter√≠sticas.
- Utiliza _MaxPooling_ para reduzir a dimens√£o da sa√≠da.
- Armazena o tensor de sa√≠da sem _pooling_ para ser utilizado nas _skip connections_.

**Se√ß√£o decoder:**
- Realiza _upsampling_ com convolu√ß√µes transpostas.
- Concatenada com as _skip connections_ do encoder.
- Refinada por um bloco convolucional.
<br>

**Defini√ß√£o da arquitetura:** Pipeline dos dados ao longo de toda a estrutura
```python
def Unet(n_classes, tensor_shape):

  input = tf.keras.layers.Input(tensor_shape) #instancia o tensor para os dados de entrada

  #se√ß√£o de contra√ß√£o:
  skip1, c1 = encoder_block(input,16) # 128x128x3 > 64x64x16
  skip2, c2 = encoder_block(c1,32) # 64x64x16 > 32x32x32
  skip3, c3 = encoder_block(c2,64) # 32x32x32 > 16x16x64
  skip4, c4 = encoder_block(c3,128) # 16x16x64 > 8x8x64

  #bottleneck
  c5 = ponte(c4,256) # 8x8x64 > 8x8x256

  #se√ß√£o de expans√£o:
  c6 = decoder_block(c5, skip4, 128) #8x8x256 > 16x16x128
  c7 = decoder_block(c6, skip3, 64) #16x16x128 > 32x32x64
  c8 = decoder_block(c7, skip2, 32) #32x32x64 > 64x64x32
  c9 = decoder_block(c8, skip1, 16) #64x64x32 > 128x128x16

  #camada de sa√≠da:
  output = tf.keras.layers.Conv2D(n_classes, (1,1), activation='softmax')(c9) #128x128x16 > 128x128x8, 8= n√∫mero de classes

  model = Model(input, output, name="U-Net")
  return model
```
- A arquitetura foi constru√≠da combinando os blocos encoder, ponte e decoder, realizando as *skip connections*.
- A camada de sa√≠da foi adicionada com uma ativa√ß√£o softmax para prever as classes dos pixels.
<br>

**Compila√ß√£o:** 

```python
#par√¢metros:
input_shape = (256,256, 1)
classes = 9
lr = 1e-3
metrics = ["accuracy"]
loss =  SparseCategoricalCrossentropy()

model = Unet(classes, input_shape)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=loss, metrics=metrics)
model.summary()

#Arquivos par asalvamento do modelo
arquivo_modelo = '/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/modelo_01_UnetUS.keras' # .h n√£o √© mais aceito
arquivo_modelo_json = '/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/modelo_01_UnetUS.json'

#Callbacks
lr_reducer = ReduceLROnPlateau( monitor='val_loss', factor = 0.9, patience = 3, verbose = 1)
early_stopper = EarlyStopping(monitor='val_loss', min_delta=0, patience = 15, verbose = 1, mode='auto')
checkpointer = ModelCheckpoint(arquivo_modelo, monitor = 'val_loss', verbose = 1, save_best_only=True)
```
**Fun√ß√µes de Perda e M√©tricas:** 
- `SparseCategoricalCrossentropy` como fun√ß√£o de perda.
- Precis√£o como m√©trica de aprendizado

**Callbacks:**
- `ReduceLROnPlateau`: Para reduzir a taxa de aprendizado automaticamente.
- `EarlyStopping`: Para interromper o treinamento caso n√£o haja melhora ap√≥s um certo n√∫mero de √©pocas.
- `ModelCheckpoint`: Para salvar o melhor modelo durante o treinamento.
<br>

**Verificando se a sa√≠da do modelo est√° do formato esperado:**

```python
#testando o modelo
import numpy as np

input_image = np.random.rand(1, 256, 256, 1)
output_image = model.predict(input_image)

input_shape = input_image.shape
output_shape = output_image.shape

print("Dimens√£o da entrada:", input_shape)
print("Dimens√£o da sa√≠da:", output_shape)
```
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1s 996ms/step
Dimens√£o da entrada: (1, 256, 256, 1)
Dimens√£o da sa√≠da: (1, 256, 256, 9)

A dimens√£o de entrada foi reduzida para 256x256 para poupar mem√≥ria.

## 3.3. Pr√©-processamento
O pr√©-processamento foi projetado de forma a permitir que seu pipeline seja aplicado a qualquer conjunto de imagens. 
### 3.3.1. Carregamento das imagens

**Fun√ß√µes:**
```python
"""esta fun√ß√£o segmenta o nome do arquivo para o img_loader ordenar o dataset
na ordem do diret√≥rio e ter correspond√™ncia entre a lista de imagens e m√°scaras"""

def natural_sort_key(s):
    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\d+)', s)]

""" img_loader recebe um caminho de diret√≥rio,uma lista vazia e  uma tupla com
dimens√µes de imagem. L√™ as imagens png ou jpg do diret√≥rio, ordena pelo nome e
 armazena a imagem na lista img_data e seu nome na lista img_names"""
def img_loader(path, img_data, size=None, rgb=True):
  #lista para o nome dos arquivos
  img_names = []

  for diretorio_path in sorted(glob.glob(path)):
    for img_path in sorted(glob.glob(os.path.join(diretorio_path, "*.[pj]*[np]*[g]*")), key=natural_sort_key): #percorre o diret√≥rio na ordem natural dos t√≠tulos de arquivo
      img = cv2.imread(img_path,
                       cv2.IMREAD_COLOR if rgb
                       else cv2.IMREAD_GRAYSCALE) #img tem 3 canais na 3 dimensao se RGB, e 1 canal se preto/branco

      if rgb:  # Corrige para formato RGB
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
      if size is not None:
        img = cv2.resize(img, size) #redimensiona conforme o par√¢metro

      img_data.append(img.astype(np.uint8)) #add a imagem na lista do parametro
      img_names.append(os.path.basename(img_path)) #add o nome do arquivo na lista de nomes

  #return img_data, img_names
  return np.array(img_data), img_names
```

- **`natural_sort_key(s)`**:
    - Essa fun√ß√£o ordena os arquivos de forma natural, tratando n√∫meros contidos nos nomes dos arquivos para garantir que a ordem seja adequada e garantir a correspond√™ncia correta entre imagens e m√°scaras.
- **`img_loader(path, img_data, size=None, rgb=True)`**:
    - Carrega as imagens de um diret√≥rio, redimensiona para as dimens√µes especificadas (`size`), e converte as imagens para o formato RGB (se necess√°rio).
    - As imagens s√£o armazenadas na lista `img_data` e os nomes dos arquivos s√£o armazenados em `img_names`.

### 3.3.2. Processamento de M√°scaras

#### 3.3.2.1. Fun√ß√µes
Fun√ß√µes utilizadas para manipular as mascaras alimentadas ao modelo e geradas por ele.

**Dicion√°rio de Cores:**
```python
def gen_dict_cores():
    mask_dict = {
        (0, 0, 0): 0,        # Preto = None
        (100, 0, 100): 1,    # Violeta = Liver (f√≠gado)
        (255, 255, 255): 2,  # Branco = Bone (Osso)
        (0, 255, 0): 3,      # Verde = Gallbladder (Ves√≠cula biliar)
        (255, 255, 0): 4,    # Amarelo = Kidney (Rins)
        (0, 0, 255): 5,      # Azul = Pancreas
        (255, 0, 0): 6,      # Vermelho = Vessels (Veias)
        (255, 0, 255): 7,    # Rosa = Spleen (Ba√ßo)
        (0, 255, 255): 8     # Azul claro = Adrenal (Gl√¢ndula Adrenal)
    }
    return mask_dict
```
 Cria um dicion√°rio que mapeia cores RGB para r√≥tulos num√©ricos. Cada cor representa uma estrutura anat√¥mica diferente, como f√≠gado, osso, ves√≠cula biliar, etc.


**Conversor de matriz RGB para Matriz de r√≥tulos:** Essa fun√ß√£o converte uma matriz RGB para uma matriz de inteiros .

```python
def RGBtoClass(rgb, dictCores):
    arr = np.zeros(rgb.shape[:2])  # Inicializa a matriz de r√≥tulos

    for color, label in dictCores.items():  # Itera sobre os pares (cor, r√≥tulo)
        color = np.array(color)  # Converte a cor para um array NumPy
        arr[np.all(rgb == color, axis=-1)] = label  # Atribui o r√≥tulo aos pixels que correspondem √† cor
    return arr
```

Para cada pixel da imagem de m√°scara, a cor √© mapeada para o r√≥tulo correspondente, substituindo a cor pelo valor do r√≥tulo. A fun√ß√£o utiliza uma opera√ß√£o matricial para a convers√£o, essencialmente fazendo uma compara√ß√£o booleana AND pixel-a-pixel. Isso faz com que o processo tenha um menor custo computacional.

<br>
**Conversor de matriz de vetores One-Hot para RGB:** Essa fun√ß√£o converte uma mascara do formato one-hot-encoded de volta para RGB. 

```python
def onehot_to_rgb(oneHot, dictCores):
    oneHot = np.array(oneHot)  # Converte para array numpy
    oneHot = np.argmax(oneHot, axis=-1)  # Seleciona o maior valor (√≠ndice)
    output = np.zeros(oneHot.shape + (3,))  # Cria a matriz RGB de sa√≠da
    oneHot = np.expand_dims(oneHot, axis=-1)  # Expande as dimens√µes

    for color, index in dictCores.items():
        output[np.all(oneHot == index, axis=-1)] = color

    return np.uint8(output)
```

A sa√≠da de um modelo de segmenta√ß√£o geralmente √© da forma (batch_size, n_classes, altura, largura). Cada √≠ndice no vetor one-hot √© mapeado para a cor correspondente definida no dicion√°rio de cores.

#### 3.3.2.2. Testando as convers√µes:
Foi feito um teste espec√≠fico para cada uma das fun√ß√µes, utilizando imagens riadas manualmente:

**Teste RGB > Matriz de r√≥tulos:**

```python
# Criando uma imagem RGB 3x3 com as cores do dicion√°rio
rgb_image = np.array([
    [(255, 255, 255), (100, 0, 100), (0, 255, 0)],
    [(0, 0, 255), (255, 0, 0), (255, 0, 255)],
    [(0, 255, 255), (255, 255, 0), (0, 0, 0)]
])

dictCores = gen_dict_cores()
# gerando matriz de rotulos]
one_hot_image = RGBtoClass(rgb_image, dictCores)

# Exibindo a imagem RGB gerada
plt.imshow(rgb_image)
plt.axis('off')
plt.show()

# Exibindo a matriz de r√≥tulos 
print("Matriz de r√≥tulos:")
print(one_hot_image)
```
![[Pasted image 20241121010505.png|250]]
Matriz de r√≥tulos (one-hot): 
[ [2. 1. 3.]
..[5. 6. 7.] 
..[8. 4. 0.]]
O resultado est√° de acordo com o dicion√°rio e com a posi√ß√£o das cores.

**Teste Matriz One-Hot > RGB:**

```python
# Definindo manualmente as classes para cada pixel da imagem 3x3
oneHot = np.zeros((3, 3, 9))  # Imagem 3x3 com 9 classes

# One-hot com as classes do dicion√°rio:

#esse resultado explicito quem o faz √© o argmax
oneHot[0, 0, 2] = 1  # Branco (Osso) (0,0)
oneHot[0, 1, 5] = 1  # Azul (Pancreas) (0,1)
oneHot[0, 2, 3] = 1  # Verde (Ves√≠cula) (0,2)

oneHot[1, 0, 1] = 1  # Violeta (F√≠gado) (1,0)
oneHot[1, 1, 6] = 1  # Vermelho (Veias) (1,1)
oneHot[1, 2, 7] = 1  # Rosa (Ba√ßo) (1,2)

oneHot[2, 0, 0] = 1  # Preto (Nada) (2,0)
oneHot[2, 1, 2] = 1  # Branco (Osso) (2,1)
oneHot[2, 2, 8] = 1  # Azul claro (Gland.Adrenal) (2,2)

# Gerando o dicion√°rio
dictCores = gen_dict_cores()

# Convertendo o vetor one-hot para a imagem RGB
rgb_image = onehot_to_rgb(oneHot, dictCores)

plt.imshow(rgb_image)
plt.axis('off')
plt.show()
print("Dimens√£o da sa√≠da do modelo:", oneHot.shape)
print("Dimens√£o da imagem RGB gerada:", rgb_image.shape)
```
![[Pasted image 20241121011106.png|250]]
Dimens√£o da sa√≠da do modelo: (3, 3, 9)
Dimens√£o da imagem RGB gerada: (3, 3, 3)

Aqui foi definido uma matriz 3x3x1 manualmente, onde cada posi√ß√£o tem o valor de um r√≥tulo definido no dicion√°rio. O modelo gera probabilidades para as dimens√µes do vetor, mas como a fun√ß√£o **argmax** colapsa o vetor para o maior valor, a representa√ß√£o das probabilidades foi deixada para o teste seguinte.

**Teste 2 One-hot > RGB:**
```python
output_image = np.random.rand(1, 4, 4, 9)  # Imagem de sa√≠da com 9 classes
output_image = np.squeeze(output_image, axis=0)  # Remove a dimens√£o do batch(so tem uma img)

# Criando o dicion√°rio
dictCores = gen_dict_cores()

# Convertendo a sa√≠da do modelo para a imagem RGB
rgb_image = onehot_to_rgb(output_image, dictCores)

# Imagem gerada
plt.imshow(rgb_image)
plt.axis('off')
plt.show()

print("Dimens√£o da sa√≠da do modelo:", output_image.shape)
print("Dimens√£o da imagem RGB gerada:", rgb_image.shape)
print(output_image)
```
![[Pasted image 20241121011727.png|250]]
Dimens√£o da sa√≠da do modelo: (4, 4, 9) 
Dimens√£o da imagem RGB gerada: (4, 4, 3)
```python
[[[0.57908881 0.33580392 0.34410017 0.23720563 0.13586635 0.05622759
   0.5862357  0.96747813 0.45280148]
  [0.8930983  0.18276845 0.62709657 0.07603959 0.67300906 0.67327464
   0.61237694 0.21165752 0.22208691]
  [0.22039128 0.94177474 0.33338415 0.51746079 0.59490881 0.01584007
   0.66245684 0.42296073 0.67942008]
  [0.93072832 0.47512061 0.56555351 0.00935921 0.76246471 0.24776436
   0.67333821 0.60336891 0.57013791]]

 [[0.67547263 0.97666166 0.48209579 0.05390186 0.98060942 0.56406069
   0.31010073 0.61860062 0.00416096]
  [0.67526767 0.77943618 0.5162135  0.08672409 0.32712662 0.58154258
   0.67918167 0.38743725 0.85377961]
  [0.10333022 0.33368111 0.2277703  0.36395461 0.17047299 0.31021793
   0.13229725 0.23419578 0.77836624]
  [0.93419304 0.82677624 0.41762912 0.72430383 0.60024323 0.73791163
   0.30090767 0.87661449 0.28460251]]

 [[0.54004565 0.31069832 0.91982634 0.50408192 0.86804815 0.66417395
   0.8121413  0.37085723 0.24864316]
  [0.47253666 0.74654554 0.94921976 0.65585023 0.01949505 0.69139072
   0.69685657 0.83856082 0.2469597 ]
  [0.89189944 0.0755049  0.35849327 0.60262315 0.28713629 0.91245026
   0.92043945 0.47336987 0.96560221]
  [0.89639344 0.1702278  0.23391221 0.46912105 0.98255504 0.47398564
   0.29338927 0.39266837 0.42605398]]

 [[0.54492246 0.48854141 0.63795983 0.95875354 0.46468765 0.97960605
   0.65112426 0.52646422 0.9889281 ]
  [0.23461664 0.77465378 0.12610709 0.07643158 0.60316224 0.59314858
   0.81460461 0.05123225 0.89071958]
  [0.53553578 0.82616564 0.98435011 0.70290783 0.75915189 0.8779408
   0.21457065 0.38440187 0.11100548]
  [0.57730195 0.68633509 0.7877986  0.79553713 0.85351347 0.06763191
   0.27510914 0.62280713 0.78067767]]]
```
Chatinho de ler, mas √© poss√≠vel verificar. A fun√ß√£o funciona muito bem.

### 3.3.3. Separa√ß√£o dos conjuntos
Com as fun√ß√µes de convers√£o definidas para as m√°scaras, agora √© poss√≠vel aplicar as transforma√ß√µes necess√°rias √†s imagens para montar os conjuntos de treino, valida√ß√£o e teste. O fluxograma abaixo ilustra todas as etapas do processamento necess√°rio para preparar os dados e fornec√™-los ao modelo:

![[PreProcessamento_Fluxograma4.png]]

**Carregando os dados**

 As imagens e as m√°scaras s√£o armazenadas em arrays `Simg_treino`, `Smask_treino`, `Simg_val`, e `Smask_val` com a fun√ß√£o img_loader. As imagens reais n√£o ser√£o utilizadas na primeira etapa do treinamento.

```python
#Carregando os dados:
dim = (256,256)
Simg_treino, Simg_treino_names = img_loader(sim_img_treino, Simg_treino,dim, False)
Smask_treino, Smask_treino_names = img_loader(sim_mask_treino,Smask_treino,dim)

Simg_val, Simg_val_names = img_loader(sim_img_val, Simg_val,dim, False)
Smask_val, Smask_val_names = img_loader(sim_mask_val,Smask_val,dim)
```

Ap√≥s o carregamento os dados s√£o plotados para verificar a correspond√™ncia entre  as imagens e m√°scaras:
![[Pasted image 20241121003533.png|center|500]]
As listas de nomes permitiram a identifica√ß√£o correta de cada arquivo e da n√£o correspond√™ncia entre as m√°scaras e imagens. A fun√ß√£o *natural_sort_keys()* foi criada em fun√ß√£o desse problema, pois os arquivos estavam sendo carregados fora de ordem.

**Concatenando o dataset real e simulado**

O dataset j√° veio separado com os subconjuntos de treino e teste que o criador utilizou, mas preferi concatenar tudo para utilizar uma divis√£o diferente.
```python
'''concatenando as imagens e mascaras de treino / teste para separar ao carregar
no  train_test_split'''
all_imgs = np.concatenate((Simg_treino, Simg_val), axis=0)
all_masks = np.concatenate((Smask_treino, Smask_val), axis=0)

all_imgname = Simg_treino_names + Simg_val_names
all_maskname = Smask_treino_names + Smask_val_names

all_imgs  = np.expand_dims(all_imgs, axis=3)
all_imgs.shape, all_masks.shape
```

Mais uma verifica√ß√£o para garantir que a correspond√™ncia entre imagens e m√°scaras n√£o foi alterada:
![[Pasted image 20241121163739.png|center| 500]]

**Convertendo todas as m√°scaras de RGB para mapas de classes:**

```python
dictCores = gen_dict_cores()
#all_img_class = [] #as imagens n√£o precisam ser convertidas para √≠ndices de classe/pixel
all_mask_class = []

"""for image in  all_imgs:
    #img_all = np.expand_dims(img_all, axis=3)
    onehotimage = RGBtoOneHot(image, dictCores)
    all_img_class.append(onehotimage)"""

for mask in  all_masks:
    onehotmask = RGBtoClass(mask, dictCores)
    all_mask_class.append(onehotmask)

#convertendo de volta para np.array
#train_img = np.array(all_img_class)
all_mask_class = np.array(all_mask_class)

#expandindo 1 dimens√£o
#train_img_all = np.expand_dims(all_img_class, axis=3)
all_mask_class = np.expand_dims(all_mask_class, axis=3)

#print("Classes √∫nicas nos pixels das imagens :", np.unique(all_img_class))
print("Classes √∫nicas nos pixels das  m√°scaras :", np.unique(all_mask_class), all_mask_class.shape)
#print("Formato das imagens :", all_img_class.shape)
```
 `Classes √∫nicas nos pixels das m√°scaras : [0. 1. 2. 3. 4. 5. 6. 7. 8.] (926, 256, 256, 1)`
  
Ao final foi feita uma verifica√ß√£o dos valores √∫nicos dos pixels e mtodo o dataset com a fun√ß√£o **np.unique**. Isso confirma que as convers√µes foram bem sucedidas.

Mais uma verifica√ß√£o, utilizando a fun√ß√£o *onehot_to_rgb* para visualizar as m√°scaras convertidas:
![[Pasted image 20241121164348.png|center|500]]
`((256, 256, 1), (256, 256, 3), (256, 256, 1), (256, 256, 3))`

Isso confirma o funcionamento das duas fun√ß√µes, *RGBtoClass* e *onehot_to_rgb*.

**Divis√£o em conjuntos de treino e valida√ß√£o:**
O conjunto de imagens `all_imgs`  e de m√°scaras `all_mask_class` foram divididos entre conjuntos de treino, valida√ß√£o e teste..

```python 
# Divis√£o inicial: 10% para teste e 90% para treino e valida√ß√£o
X_train_val, X_test, y_train_val, y_test = train_test_split(
    all_imgs, all_mask_class, test_size=0.1, random_state=0, shuffle=False
)

# Segunda divis√£o: 15% para valida√ß√£o e 85% para teste
X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.15, random_state=0, shuffle=False
)

# tamanhos dos conjuntos
print(f"Treino: {len(X_train)}, Valida√ß√£o: {len(X_val)}, Teste: {len(X_test)}")```

**Normaliza√ß√£o dos dados:**
Modelos de deep learning trabalham melhor com valores dos dados entre 0 e 1. Os dados foram normalizados para auxiliar na converg√™ncia da rede.
```python
#normalizando as imagens
X_train = X_train.astype('float32') / 255.0
X_val = X_val.astype('float32') / 255.0
X_test = X_test.astype('float32') / 255.0

y_train = y_train.astype(np.int32)
y_val = y_val.astype(np.int32)
y_test = y_test.astype(np.int32)
```

Mais um teste do dataset - por que n√£o? - Verificando todas as caracter√≠sticas dos dados:
```python
#Teste de sanidade
print("Classes √∫nicas em y_train", np.unique(y_train), y_train.shape, y_train.dtype)
print("Classes √∫nicas em y_val", np.unique(y_val), y_val.shape, y_val.dtype)
print("Classes √∫nicas em y_test:", np.unique(y_test), y_test.shape, y_test.dtype)

print("Formato de X_train:", X_train.shape, X_train.dtype)
print("Formato de X_val:", X_val.shape, X_val.dtype)
print("Formato de X_test", X_test.shape, X_test.dtype)
```
`Classes √∫nicas em y_train [0 1 2 3 4 5 6 7 8] (833, 256, 256, 1) int32` 
`Classes √∫nicas em y_test: [0 1 2 3 4 5 6 7 8] (93, 256, 256, 1) int32` 
`Formato de X_train: (833, 256, 256, 1) float32` 
`Formato de X_test (93, 256, 256, 1) float32`

E outra visualiza√ß√£o do dataset dividido, normalizado e com os tipos convertidos:
![[Pasted image 20241121165904.png|center|500]]

As m√°scaras foram convertidos para inteiros na tentativa de implementar um balanceamento no dataset, embora n√£o tenha sido implementado at√© o momento(n√£o consegui resolver o erro):
```python
from sklearn.utils import class_weight
class_weights = class_weight.compute_class_weight(class_weight='balanced',
                                                  classes=np.unique(y_train),
                                                  y=y_train.flatten())  # Flatten y_train to 1D

print("Os pesos das classes s√£o:", class_weights)
#dicion√°rio com pesos das classes para balanceamento
class_weights = {
    class_weights[0]: 0,
    class_weights[1]: 1,
    class_weights[2]: 2,
    class_weights[3]: 3,
    class_weights[4]: 4,
    class_weights[5]: 5,
    class_weights[6]: 6,
    class_weights[7]: 7,
    class_weights[8]: 8
}
```
`Os pesos das classes s√£o: [1.33613583e-01 9.49245410e-01 4.97438956e+01 2.72413430e+01 6.13989077e+00 1.34157296e+01 3.94479946e+01 7.19208821e+00 2.73934015e+02]`
A classe 02 (f√≠gado, roxo) era a classe predominante depois do preto, e seu peso foi significativamente reduzido em rela√ß√£o aos demais.

**Fun√ß√µes para salvamento**
Foi necess√°rio salvar dataset dividido (como np.array) e evitar processar tudo novamente (o colab estava quebrando por causa da mem√≥ria T.T ):

```python
def save_dataset(X_train, X_test, y_train, y_test, save_dir="dataset"):

    os.makedirs(save_dir, exist_ok=True)

    np.save(os.path.join(save_dir, "X_train.npy"), X_train)
    np.save(os.path.join(save_dir, "X_test.npy"), X_test)
    np.save(os.path.join(save_dir, "y_train.npy"), y_train)
    np.save(os.path.join(save_dir, "y_test.npy"), y_test)
    print(f"Dataset salvo em {save_dir}")
    
def load_dataset(save_dir="dataset"):
 X_train = np.load(os.path.join(save_dir, "X_train.npy"))
  X_test = np.load(os.path.join(save_dir, "X_test.npy"))
  y_train = np.load(os.path.join(save_dir, "y_train.npy"))
  y_test = np.load(os.path.join(save_dir, "y_test.npy"))

  print(f"Dataset carregado de {save_dir}")
  return X_train, X_test, y_train, y_test
```

Executando o salvamento:
```python
save_dataset(X_train, X_test, y_train, y_test, save_dir = "/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/Dataset_UltrassomAbdominal")
```

Al√©m disso, seguindo a recomenda√ß√£o de um colega para verificar se as imagens est√£o ok depois de todo o processo, uma materializa√ß√£o das imagens em si:

```python
import imageio
def salvar_imagens_em_diretorio(data, diretorio_destino):
    """
    Salva as imagens de um array NumPy em um diret√≥rio espec√≠fico.
    """
    # Criar o diret√≥rio se ele n√£o existir
    os.makedirs(diretorio_destino, exist_ok=True)

    # Salvar as imagens
    for i in range(data.shape[0]):
        nome_arquivo = f"imagem_{i}.jpg"
        caminho_completo = os.path.join(diretorio_destino, nome_arquivo)
        imageio.imwrite(caminho_completo,data[i])
        #imageio.imwrite(caminho_completo, onehot_to_rgb(data[i],dictCores))
```

```python
salvar_imagens_em_diretorio(X_train, "/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/Dataset_UltrassomAbdominal/X_train")
salvar_imagens_em_diretorio(X_test, "/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/Dataset_UltrassomAbdominal/X_test")
salvar_imagens_em_diretorio(y_train, "/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/Dataset_UltrassomAbdominal/y_train")
salvar_imagens_em_diretorio(y_test, "/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/Dataset_UltrassomAbdominal/y_test")
```
## 4. Treinamento

**Carregando o dataset processado:**
O primeiro passo para iniciar o treinamento, √© carregar o dataset como np.array:
```python
#carregando os conjuntos de dados
X_train, X_test, y_train, y_test = load_dataset(save_dir="/content/drive/MyDrive/Colab Notebooks/SegementacÃßaÃÉo_Abdominal/Dataset_UltrassomAbdominal")
```

**Convers√£o das m√°scaras para one-hot:**
A fun√ß√£o **to_categorical** converterte os √≠ndices de classe inteiros para  vetores one-hot, transformando as m√°scaras para o formato (h, w, 9).
```python 
y_train_cat = to_categorical(y_train, num_classes = 9)
y_test_cat = to_categorical(y_test, num_classes = 9)
```
Do segundo teste em diante  esta convers√£o n√£o foi mais utilizada, para poupar o uso de mem√≥ria da plataforma. Por conta disso a fun√ß√£o de perda do modelo mudou de *categorical_cross_entropy* para *sparse_categorical_cross_entropy*, que funciona com o formato (h, w, 1).

**Treinamento**

```python
history = model.fit(X_train, y_train,
                    batch_size=16,
                    verbose=1,
                    epochs = 50,
                    validation_data=(X_test, y_test),
                    callbacks=[lr_reducer, early_stopper, checkpointer],
                    shuffle=False)
```

Para o treinamento foi utilizado um batch_size de 16, 50 √©pocas e os callbacks definidos anteriormente. O treinamento com balanceamento do dataset n√£o funcionou. O treinamento durou apenas 16 √©pocas, e logo foi encerrado pelo ***early_stopping***.
## 5. Resultados e Discuss√£o:
Nesta se√ß√£o ser√£o descritos os testes, avalia√ß√µes e m√©tricas de avalia√ß√£o utilizadas para analisar o desempenho do modelo de segmenta√ß√£o U-Net aplicado nas imagens de ultrassom.
### 5.1. M√©tricas de avalia√ß√£o
Para a avalia√ß√£o do modelo, foram utilizadas as seguintes m√©tricas:
#### 5.1.1. Acur√°cia
A acur√°cia √© a m√©trica mais comum, indicando a porcentagem de pixels corretamente classificados. Em problemas de segmenta√ß√£o ela n√£o √© a mais indicada, especialmente para datasets desbalanceados pois ela ser√° enviesada pelos **verdadeiros negativos**.

![[Pasted image 20241122124533.png|center]]

![[Pasted image 20241122124947.png|center]]

- **TP**: Pixels corretamente classificados.
- **TN**: Pixels corretamente n√£o classificados
- **FP**: Pixels incorretamente classificados.
- **FN**: Pixels que foram ignorados
#### 5.1.2. IoU(Intersection Over Union) ou Jaccard Coefficient
A **IoU** √© a m√©trica mais adequada para avaliar modelos de segmenta√ß√£o, especialmente os multiclasse. Ela calcula o percentual da predi√ß√£o que coincide com o **Ground Truth**. A interse√ß√£o¬†(ùê¥‚à©ùêµ) s√£o os pixels que pertencem √† predi√ß√£o e √† m√°scara ground truth, e a uni√£o (ùê¥‚à™ùêµ) s√£o todos os pixels contidos nos dois.


![[Pasted image 20241122131328.png|center]]

**Exemplo**
Ground truth e predi√ß√£o:
![[Pasted image 20241122125247.png]]
Intersec√ß√£o e Uni√£o:
![[Pasted image 20241122125253.png]]


Pixel-a-Pixel:
![[Pasted image 20241122124700.png|center]]

- **TP**: Pixels corretamente classificados.
- **FP**: Pixels incorretamente classificados.
- **FN**: Pixels que foram ignorados

![[Pasted image 20241122125209.png|center]]

C√°lculo da **Mean IoU** utilizando a fun√ß√£o nativa do Keras:
```python
#avaliando pela metrica "intersection over union"
from keras.metrics import MeanIoU
n_classes = 9
IOU_keras = MeanIoU(num_classes=n_classes)
IOU_keras.update_state(y_test[:,:,:,0], y_pred_argmax)
print("Mean IoU =", IOU_keras.result().numpy())
```
¬†¬†**Mean IoU**¬†(IoU m√©dio) √© calculada como a m√©dia dos IoUs de todas as classes presentes:

### 5.2.  Testes e Avalia√ß√£o
Nesta se√ß√£o s√£o descritos os tipos de testes realizados para avaliar o funcionamento da implementa√ß√£o e a capacidade de generaliza√ß√£o do modelo. A avalia√ß√£o foi realizada em duas etapas principais:  testes de implementa√ß√£o e  generaliza√ß√£o com o dataset sint√©tico, e testes de generaliza√ß√£o com o dataset de imagens reais. Os primeiros testes foram feitos durante a implementa√ß√£o, com o conjunto de valida√ß√£o, para verificar se o modelo e o pipeline de processamento estavam funcionando corretamente, seguidos de um novo  teste utilizando um conjunto nunca visto pelo modelo. Em seguida, novos modelos foram treinados com o dataset de imagens reais, que por ser muito escasso, demandou mudan√ßas significativas nas estrat√©gias de treinamento e arquitetura da rede.

### ==Dataset Sint√©tico==
#### 5.2.1. Teste1: 

**Resultados**
![[image.png]]
No primeiro teste eu n√£o fazia ideia de onde estava o erro. A materializa√ß√£o dos conjuntos revelou problemas no pipeline do pr√© processamento. Os principais erros identificados foram:

-  As imagens de ultrassom foram convertidas da escala de cinza para os √≠ndices de classe, quando n√£o deveriam.
- O modelo foi compilado com *categorical_cross_entropy*, mas m√°scaras n√£o estavam no formato esperado, como matrizes de vetores one-hot.

Esses erros comprometeram o funcionamento do modelo mas deram pistas para o ajuste do processo.

#### 5.2.2. Teste 2

**Resultados:** 
![[teste2 1.jpeg|center]]
![[teste2a 2.jpg|center]]
![[pred1.png|center|]]

Ap√≥s corrigir os erros identificados no primeiro teste, os resultados foram med√≠ocres mas forneceram informa√ß√µes importantes:

- A U-net implementada passou a treinar corretamente e realizar predi√ß√µes.
- Apenas o conjunto de teste foi normalizado, causando inconsist√™ncias entre os dados de treino e teste.
- O dataset est√° desbalanceado, com algumas classes tendo mais representatividade que outras, como o f√≠gado, por exemplo.

Apesar disso, o modelo foi capaz de aprender algumas caracter√≠sticas gerais das imagens ‡¥¶‡µç‡¥¶‡¥ø(Àµ ‚Ä¢ÃÄ ·¥ó - Àµ ) ‚úß.

#### 5.2.3.  Teste 3

- 832 Imagens de treino;
- 92 imagens de valida√ß√£o;

##### **Resultados:**
1![[pred2-11.png]]
2![[pred2-5.png]]3![[pred2-4.png]]
4![[pred2-13.png]]
5![[pred2-1.png]]
Ap√≥s as corre√ß√µes, o modelo foi capaz de aprender as caracter√≠sticas das imagens e conseguiu gerar m√°scaras muito pr√≥ximas das originais, classificando corretamente v√°rios √≥rg√£os. Algumas defici√™ncias e dificuldades foram identificadas:

- O modelo confundiu algumas classes e gerou m√°scaras de cores trocadas.
- Mesmo classificando corretamente, o formato dos √≥rg√£os est√° distorcido em algumas predi√ß√µes.
- As classes menos representadas foram as mais afetadas pelos erros de classifica√ß√£o e distor√ß√£o, destacando o desbalanceamento do dataset.

Apesar disso, o modelo executou corretamente a tarefa proposta e seu funcionamento foi validado. ‡¥¶‡µç‡¥¶‡¥ø ÀâÕàÃÄÍí≥ÀâÕàÃÅ )‚úß
##### **Avalia√ß√£o:** 

**Perda de treino e valida√ß√£o:**
![[Pasted image 20241121172301.png|center|350]]

- A perda de treinamento decresce continuamente,  indicando que o modelo ajustou os pesos e aprendeu com os dados.
- A oscila√ß√£o e valor da perda de valida√ß√£o foram maiores que os do treinamento, indicando ***overfitting*** e talvez problemas com ru√≠dos nas imagens.

**Acur√°cia de treino e valida√ß√£o:**
![[Pasted image 20241121172313.png|center|350]]

- A acur√°cia de treinamento foi de 98% e com crescimento constante, indicando que o modelo conseguiu identificar os padr√µes nos dados de treinamento.
- Acur√°cia de valida√ß√£o oscilou muito e seu valor foi menor que a de treinamento, indicando ***overfitting***.

**Intersection over Union:**

`Mean IoU = 0.33217388`

**M√©tricas de treinamento:**
`- accuracy: 0.9306` 
`- loss: 0.3087` 
`- Accuracy is = 93.20508241653442 %`

#### 5.2.4. Teste de generaliza√ß√£o
Utilizando um conjunto de teste com imagens n√£o usadas no treinamento.
- 708 Imagens de treino;
- 125 Imagens valida√ß√£o;
- 93 Imagens de teste;
##### Resultados:
Uma sele√ß√£o de resultados bons e ruins
1![[plot_35.png]]
2![[plot_14.png]]
3![[plot_47.png]]
4![[plot_27.png]]
5![[plot_72.png]]
6![[plot_66.png]]

- **Predi√ß√µes 1 e 2:** Resultados satisfat√≥rios. O modelo foi capaz de gerar m√°scaras pr√≥ximas as reais, com a localiza√ß√£o e classifica√ß√£o corretas.
- **Predi√ß√µes 3 e 4:** Resultados medianos. O modelo gerou boas m√°scaras para algumas classes, e outras foram ignoradas. Tamb√©m houveram erros de classifica√ß√£o.
- **Predi√ß√µes 5 e 6:** Resultados ruins. O modelo confundiu e ignorou algumas classes, e gerou m√°scaras inexistentes.

Mesmo nos melhores resultados, as m√°scaras geradas tem bordas distorcidas em rela√ß√£o ao ground thuth. Os piores resultados podem ter sido influenciados pela natureza ruidosa das imagens, e pelo dataset desbalanceado.
##### Avalia√ß√£o: 

**Perda de treino e valida√ß√£o:**
![[Pasted image 20241121215646.png|center|350]]
- A perda de treino √© baixa durante todo o treinamento, um forte ind√≠cio de overfitting.
- A queda da perda no treino de valida√ß√£o indica que o modelo est√° aprendendo bem no in√≠cio.

**Acur√°cia de treino e valida√ß√£o:**
![[Pasted image 20241121215655.png|center|350]]
- A diferen√ßa entre a acur√°cia de treino e valida√ß√£o refor√ßa o overfitting.

O treinamento deste teste, embora com a mesma acur√°cia, foi melhor que o anterior. O aumento do conjunto de valida√ß√£o certamente influenciou nisso, pois foi o √∫nico par√¢metro alterado.

**M√©tricas de treinamento:**
`-accuracy: 0.9322` 
`-loss: 0.2672 
`-Accuracy is = 93.42482089996338 %`

**Intersection over Union:**
`Mean IoU val = 0.3306403`
`Mean IoU TESTE = 0.3960948`

Apesar de a acur√°cia estar alta, ela *n√£o √© a **m√©trica ideal** para problemas de segmenta√ß√£o, por ser influenciada pelo desbalanceamento  dos dados. Como o modelo infere para cada pixel da imagem, ele acaba classificando corretamente as classes majorit√°rias, como o fundo preto que √© a classe mais bem representada no dataset.

A m√©trica correta - **IoU(Intersection over Union)** - est√° longe do ideal, apenas 39% da √°rea predita pelo  modelo coincide corretamente com o *ground truth*. Os 61% restantes correspondem a falsos positivos (√°reas preditas que n√£o existem no ground truth) e falsos negativos (√°reas do ground truth que n√£o foram preditas).

### Dataset Real

#### Teste 1: igual aos testes sinteticos

### Fun√ß√µes de perda customizadas

#### K-Fold Cross validation
### 5.3. Melhorias propostas

Para melhorar os resultados do modelo, as seguintes implementa√ß√µes podem ser feitas:

1. **Data Augmentation:** O trabalho ''Automated Deep Learning-Based Finger Joint Segmentation in 3-D Ultrasound Images With Limited Dataset'''  sugere os seguintes par√¢metros de data augmentation para datasets de ultrassom;

	**Transforma√ß√µes geom√©tricas aleat√≥rias:**
    
    - Transla√ß√£o (horizontal e vertical) dentro de [-30%, 30%].
    - Rota√ß√£o dentro de [-15¬∞, 15¬∞].
    - **Cisalhamento (Shear) dentro de [-5¬∞, 5¬∞]**.
   
	**Varia√ß√µes aleat√≥rias de brilho e contraste:**
    
    - **Brilho**¬†e¬†**Contraste**¬†ajustados aleatoriamente para simular diferentes configura√ß√µes de equipamentos de ultrassom.
    
![[Pasted image 20250315172552.png]]
![[Pasted image 20250315172616.png]]
![[Pasted image 20250315172629.png]]
![[Pasted image 20250315172709.png]]

2. **Balanceamento dos dados:** Aplicar pesos de classe na fun√ß√£o para dar mais import√¢ncia √†s classes menos representadas.
   
3. **Testar  diferentes fun√ß√µes de perda:
	- **Jaccard Coefficient Loss:**¬†Otimiza os pesos utilizando a m√©trica Intersection over Union. √ötil para datasets desbalanceados.
	- **Dice Coeficient Loss:**

4. **Testar otimizadores diferentes como AdamW**

5. **Diferentes visualiza√ß√µes de resultados  e m√©tricas:**
   - Visualiza√ß√£o da intersec√ß√£o entre a m√°scara gerada e o ground truth, e imagem original;
   - Visualiza√ß√£o geral das predi√ß√µes com uma matriz de confus√£o utilizando um *threshhold* para o valor de IoU;
   - Grafico da curva AUC-ROC;
   - Gera√ß√£o de legendas na predi√ß√£o de acordo com as classes presentes na imagem;
## 6. Trabalhos futuros

Neste projeto foi desenvolvida e treinada uma arquitetura U-Net para segmenta√ß√£o sem√¢ntica multiclasse em imagens de ultrassonografia, simuladas a partir de dados volum√©tricos de tomografia computadorizada. A evolu√ß√£o do projeto seria, naturalmente, sua adapta√ß√£o para processar as imagens de reais de ultrassom do dataset original. Isso pode ser alcan√ßado utilizando a t√©cnica de **transfer learning** aproveitando os pesos treinados na etapa atual, juntamente com  data augmentation para contornar o problema da pequena amostra de imagens anotadas. Diferentes varia√ß√µes da U-net tamb√©m podem ser implementadas, como **Attention U-net** que utiliza mecanismos de aten√ß√£o para focar em regi√µes importantes da imagem, ou a utiliza√ß√£o de fun√µes de perda customizadas..

Outra possibilidade seria a adapta√ß√£o da rede para processar dados volum√©tricos e realizar **segmenta√ß√£o sem√¢ntica 3D**. O pipeline desenvolvido no projeto √© flex√≠vel e permite o processamento de diferentes conjuntos de dados para resolver problemas variados de segmenta√ß√£o.

